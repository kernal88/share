almost all locks are implemented in userspace, for efficient

multithread then fork, child process clones all locks but discards all threads
多线程程序 fork 前，应该由发起 fork 的线程 lock 所有子进程可能用到的锁，fork 后，把它们一一 unlock 
当然，这样的做法就隐含了锁的次序。如果次序和平时不同，那么就会死锁。

all mutual:
    spinlock        spin
    semaphore       block then process context switch
writer to reader:
    rwlock          spin, equality, but writer may spin if there's at least one reader
    seqlock         spin, equality, reader won't be blocked, writing and reading may happen simultanously, if changed during reading, read again
    rcu             spin, reader is prior, just for pointed contents, not pointer itself

spin_lock_irq & spin_lock_irqsave:
在使用spin_lock_irq和spin_unlock_irq的情况下，完全可以用spin_lock_irqsave和spin_unlock_irqrestore取代，那具体应该使用哪一个也需要依情况而定，
如果可以确信在对共享资源访问前中断是使能的，那么使用spin_lock_irq更好一些，因为它比spin_lock_irqsave要快一些，但是如果你不能确定是否中断使能，
那么使用spin_lock_irqsave和spin_unlock_irqrestore更好，因为它将恢复访问共享资源前的中断标志而不是直接使能中断。
当然，有些情况下需要在访问共享资源时必须中断失效，而访问完后必须中断使能，这样的情形使用spin_lock_irq和spin_unlock_irq最好

rcu: 
from Documentation/RCU/whatisRCU.txt
more example in listRCU.txt arrayRCU.txt checklist.txt
	         CPU 0                  CPU 1                 CPU 2
	     ----------------- ------------------------- ---------------
	 1.  rcu_read_lock()
	 2.                    enters synchronize_rcu()
	 3.                                               rcu_read_lock()
	 4.  rcu_read_unlock()
	 5.                     exits synchronize_rcu()
	 6.                                              rcu_read_unlock()

	To reiterate, synchronize_rcu() waits only for ongoing RCU
	read-side critical sections to complete, not necessarily for
	any that begin after synchronize_rcu() is invoked.

	The call_rcu() API is a callback form of synchronize_rcu(),
	and is described in more detail in a later section.  Instead of
	blocking, it registers a function and argument which are invoked
	after all ongoing RCU read-side critical sections have completed.
	This callback variant is particularly useful in situations where
	it is "illegal to block" or where "update-side performance is
	critically important".

    rcu_assign_pointer(p, v);
 * this call documents which pointers will be dereferenced by RCU read-side code.
    rcu_assign_pointer() is most frequently used indirectly, via
    the _rcu list-manipulation primitives such as list_add_rcu().

    rcu_dereference(p)
	Common coding practice uses rcu_dereference() to copy an
	RCU-protected pointer to a local variable, then dereferences
	this local variable, for example as follows:

		p = rcu_dereference(head.next);
		return p->data;

	However, in this case, one could just as easily combine these
	into one statement:

		return rcu_dereference(head.next)->data;

    /* write part also need lock for concurrence */
	void foo_update_a(int new_a)
	{
		struct foo *new_fp;
		struct foo *old_fp;

		new_fp = kmalloc(sizeof(*new_fp), GFP_KERNEL);
		spin_lock(&foo_mutex); 
		old_fp = gbl_foo;
		*new_fp = *old_fp;
		new_fp->a = new_a;
		rcu_assign_pointer(gbl_foo, new_fp);
		spin_unlock(&foo_mutex);
		synchronize_rcu();
		kfree(old_fp);
	}
    /* read part need no lock */
	int foo_get_a(void)
	{
		int retval;

		rcu_read_lock();
		retval = rcu_dereference(gbl_foo)->a;
		rcu_read_unlock();
		return retval;
	}

#define preempt_disable() \
do { \          
    inc_preempt_count(); \
    barrier(); \
} while (0)
#define inc_preempt_count()    add_preempt_count(1)
#define add_preempt_count(val) do { preempt_count() += (val); } while (0)
#define preempt_count()        (current_thread_info()->preempt_count)

dead lock:
1.  simplest circumstance
    Try to aquire the same lock on the same core. If 2 cores compete, one executes the other waits, no problem.
    Avoid interrupting by another executing thread, like hard-int, softirqs, higher-priority processes, they may try to aquire the locked spinlock leading to dead lock.
    spin_lock may disable preemption whatever in UP or SMP, so if share sth. with interrupts, spin_lock_bh/spin_lock_irq/spin_lock_irqsave.
    The last possibility is, direct or indirect schedule during lock() and unlock(), for example del_timer_sync().

    Mutex doesn't have this issue in process context, the second time the same core try to lock the locked mutex, sleep rather than spin, which avoid dead lock.

    Make a conclusion, using spin_lock in process context should be paid more attention, because of the indirect process schedule. That's why mutex are always prefered in 
    process context.

    Spinlock can avoid priority-inversion, use priority-ceiling, in reality when preempt_disable(), there's only 1 priority level in the system.

2.  fatal hug
